{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH=\"/home/visit/learn_tf/ch5/model\"\n",
    "MODEL_NAME=\"mnist_model\"\n",
    "'''\n",
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"/home/visit/learn_tf/ch5/model\"\n",
    "MODEL_NAME = \"mnist_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name = \"x-input\")\n",
    "    y_= tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name = \"y-input\")\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits = y, \n",
    "        labels = tf.argmax(y_, 1))\n",
    "    \n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                              global_step,\n",
    "                                              mnist.train.num_examples / BATCH_SIZE ,\n",
    "                                              LEARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name = 'train')\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x : xs, y_ : ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step = global_step)\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/visit/learn_tf/ch5/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/visit/learn_tf/ch5/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/visit/learn_tf/ch5/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/visit/learn_tf/ch5/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), loss on training batch is 2.86196.\n",
      "After 1000 training step(s), loss on training batch is 0.23306.\n",
      "After 2000 training step(s), loss on training batch is 0.156007.\n",
      "After 3000 training step(s), loss on training batch is 0.161985.\n",
      "After 4000 training step(s), loss on training batch is 0.127048.\n",
      "After 5000 training step(s), loss on training batch is 0.106127.\n",
      "After 6000 training step(s), loss on training batch is 0.10239.\n",
      "After 7000 training step(s), loss on training batch is 0.0921367.\n",
      "After 8000 training step(s), loss on training batch is 0.0805168.\n",
      "After 9000 training step(s), loss on training batch is 0.0774883.\n",
      "After 10000 training step(s), loss on training batch is 0.0686281.\n",
      "After 11000 training step(s), loss on training batch is 0.0614156.\n",
      "After 12000 training step(s), loss on training batch is 0.0570973.\n",
      "After 13000 training step(s), loss on training batch is 0.0550252.\n",
      "After 14000 training step(s), loss on training batch is 0.0504887.\n",
      "After 15000 training step(s), loss on training batch is 0.0495578.\n",
      "After 16000 training step(s), loss on training batch is 0.0498208.\n",
      "After 17000 training step(s), loss on training batch is 0.0465946.\n",
      "After 18000 training step(s), loss on training batch is 0.0482028.\n",
      "After 19000 training step(s), loss on training batch is 0.0407827.\n",
      "After 20000 training step(s), loss on training batch is 0.0440393.\n",
      "After 21000 training step(s), loss on training batch is 0.0440457.\n",
      "After 22000 training step(s), loss on training batch is 0.0378827.\n",
      "After 23000 training step(s), loss on training batch is 0.0395451.\n",
      "After 24000 training step(s), loss on training batch is 0.0401054.\n",
      "After 25000 training step(s), loss on training batch is 0.0347992.\n",
      "After 26000 training step(s), loss on training batch is 0.037419.\n",
      "After 27000 training step(s), loss on training batch is 0.034953.\n",
      "After 28000 training step(s), loss on training batch is 0.0351999.\n",
      "After 29000 training step(s), loss on training batch is 0.0351567.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets('/home/visit/learn_tf/ch5/mnist', one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
